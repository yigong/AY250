{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pickle"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ls"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\u001b[34m50_categories\u001b[m\u001b[m/\r\n",
        "Untitled0.ipynb\r\n",
        "\u001b[34mclassifier\u001b[m\u001b[m/\r\n",
        "hw_4-machine-learning-parallel-strawman.ipynb\r\n",
        "hw_4-machine-learning-parallel-strawman.py\r\n",
        "hw_4-machine-learning-sample.py\r\n",
        "hw_4-machine-learning.pdf\r\n",
        "hw_4_parallel_sample.py\r\n",
        "pred.ipynb\r\n",
        "random_forest.ipynb\r\n",
        "test.ipynb\r\n",
        "\u001b[34mtest_pics\u001b[m\u001b[m/\r\n",
        "trained_classifier_test.p\r\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pkl_file = open('trained_classifier_test.p', 'rb')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data1 = pickle.load(pkl_file)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print data1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "RandomForestClassifier(bootstrap=True, compute_importances=True,\n",
        "            criterion=gini, max_depth=None, max_features=auto,\n",
        "            min_density=0.1, min_samples_leaf=1, min_samples_split=2,\n",
        "            n_estimators=50, n_jobs=-1, oob_score=False, random_state=None,\n",
        "            verbose=0)\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def run_final_classifier(path):\n",
      "    # import\n",
      "#!/usr/bin/env python\n",
      "import matplotlib.pyplot as plt\n",
      "from glob import glob\n",
      "import numpy as np\n",
      "import scipy.ndimage as ndimg\n",
      "import skimage.filter as filter\n",
      "import skimage.transform as transform\n",
      "import re\n",
      "from os import listdir\n",
      "from multiprocessing import Pool, cpu_count\n",
      "from pylab import imread\n",
      "from time import time\n",
      "import pickle\n",
      "\n",
      "def row(img2d):\n",
      "    return np.shape(img2d)[0]\n",
      "    \n",
      "def col(img2d):\n",
      "    return np.shape(img2d)[1]\n",
      "\n",
      "def layer_mean(img2d):\n",
      "    img1d = img2d[img2d<255]\n",
      "    return img1d.mean()\n",
      "\n",
      "def hist_max(img2d):\n",
      "    img1d = img2d[img2d<255]\n",
      "    [n, bins, patches]= plt.hist(img1d, np.arange(255))\n",
      "    return bins[n.argmax()]\n",
      "\n",
      "def edge_length(img2d):\n",
      "    \"gray input\"\n",
      "    imgsize = float((img2d.shape[0]*img2d.shape[1]))\n",
      "    med_filter = ndimg.median_filter(img2d, size = (5,5))\n",
      "    edges = filter.canny(med_filter,3)\n",
      "    return edges.sum()/imgsize\n",
      "\n",
      "def edge_sobel_h(img2d):\n",
      "    \"gray input\"\n",
      "    imgsize = float((img2d.shape[0]*img2d.shape[1]))\n",
      "    med_filter = ndimg.median_filter(img2d, size = (5,5))\n",
      "    edges_h = filter.hsobel(med_filter/255.)\n",
      "    return edges_h.sum()/imgsize\n",
      "\n",
      "def edge_sobel_v(img2d):\n",
      "    \"gray input\"\n",
      "    imgsize = float((img2d.shape[0]*img2d.shape[1]))\n",
      "    med_filter = ndimg.median_filter(img2d, size = (5,5))\n",
      "    edges_v = filter.vsobel(med_filter/255.)\n",
      "    return edges_v.sum()/imgsize\n",
      "\n",
      "def edge_sobel(img2d):\n",
      "    \"gray input\"\n",
      "    imgsize = float((img2d.shape[0]*img2d.shape[1]))\n",
      "    med_filter = ndimg.median_filter(img2d, size = (5,5))\n",
      "    edges = filter.sobel(med_filter/255.)\n",
      "    return edges.sum()/imgsize\n",
      "\n",
      "def houghLine(img2d):\n",
      "    \"gray input\"\n",
      "    med_filter = ndimg.median_filter(img2d, size = (5,5))\n",
      "    edges = filter.sobel(med_filter/255.)\n",
      "    [H,theta,distances] = transform.hough_line(edges);\n",
      "    imgsize = float(len(theta)*len(distances))\n",
      "    return H.sum()/imgsize\n",
      "\n",
      "def cate_extract(image_path):\n",
      "    cate_temp = image_path.replace(MYDIRECTORY,'')\n",
      "    cate = re.search(r'/.+?/', cate_temp)\n",
      "    return cate_map[cate.group()[1:-1]]\n",
      "\n",
      "def features(image_path):\n",
      "    #red mean\n",
      "    img2d = ndimg.imread(image_path)\n",
      "    img2d_gray = ndimg.imread(image_path, flatten= True)\n",
      "    \n",
      "    row_n = row(img2d_gray)\n",
      "    col_n = col(img2d_gray)\n",
      "    \n",
      "    red_mean = layer_mean(img2d[...,0])\n",
      "    green_mean = layer_mean(img2d[...,1])\n",
      "    blue_mean = layer_mean(img2d[...,2])\n",
      "    gray_mean = layer_mean(img2d_gray)\n",
      "    \n",
      "    red_most = hist_max(img2d[...,0])\n",
      "    green_most = hist_max(img2d[...,1])\n",
      "    blue_most = hist_max(img2d[...,2])\n",
      "    gray_most = hist_max(img2d_gray)\n",
      "    \n",
      "    length = edge_length(img2d_gray)\n",
      "    sobel_h = edge_sobel_h(img2d_gray)\n",
      "    sobel_v = edge_sobel_v(img2d_gray)\n",
      "    sobel = edge_sobel(img2d_gray)\n",
      "    hough = houghLine(img2d_gray)\n",
      "    \n",
      "    cate = cate_extract(image_path)\n",
      "    \n",
      "    return list([row_n, col_n, red_mean, green_mean, blue_mean, gray_mean,\\\n",
      "                     red_most,green_most,blue_most, gray_most,\\\n",
      "                     length, sobel_h, sobel_v, sobel, hough, cate])\n",
      "    \n",
      "#!/usr/bin/env python\n",
      "\"\"\"\n",
      "AY 250 - Scientific Research Computing with Python\n",
      "Homework Assignment 4 - Parallel Feature Extraction Example\n",
      "Author: Christopher Klein, Joshua Bloom\n",
      "\"\"\"\n",
      "## CHANGE THIS NEXT LINE!\n",
      "MYDIRECTORY = path\n",
      "\n",
      "# FUNCTION DEFINITIONS\n",
      "# Quick function to divide up a large list into multiple small lists, \n",
      "# attempting to keep them all the same size. \n",
      "def split_seq(seq, size):\n",
      "        newseq = []\n",
      "        splitsize = 1.0/size*len(seq)\n",
      "        for i in range(size):\n",
      "            newseq.append(seq[int(round(i*splitsize)):\n",
      "                int(round((i+1)*splitsize))])\n",
      "        return newseq\n",
      "# Our simple feature extraction function. It takes in a list of image paths, \n",
      "# does some measurement on each image, then returns a list of the image paths\n",
      "# paired with the results of the feature measurement.\n",
      "def extract_features(image_path_list):\n",
      "    feature_list = []\n",
      "    for image_path in image_path_list:\n",
      "        image_array = imread(image_path)\n",
      "        ft = features(image_path)\n",
      "        \n",
      "        #ft = image_array.shape # This feature is simple. You can modify this\n",
      "        # code to produce more complicated features and to produce multiple\n",
      "        # features in one function call.\n",
      "        feature_list.append(ft)\n",
      "    return feature_list\n",
      "### Main program starts here ###################################################\n",
      "# We first collect all the local paths to all the images in one list\n",
      "image_paths = []\n",
      "categories = listdir(MYDIRECTORY)\n",
      "\n",
      "cate_map = dict(zip(categories, range(len(categories))))\n",
      "for category in categories[2:3]:\n",
      "    image_names = listdir(MYDIRECTORY  + \"/\" + category)\n",
      "    for name in image_names[1:10]:\n",
      "        image_paths.append(MYDIRECTORY + \"/\" + category + \"/\" + name)\n",
      "ip = image_paths    \n",
      "image_paths = image_paths\n",
      "print (\"There should be 4244 images, actual number is \" + \n",
      "    str(len(image_paths)) + \".\")\n",
      "# Then, we run the feature extraction function using multiprocessing.Pool so \n",
      "# so that we can parallelize the process and run it much faster.\n",
      "#numprocessors = cpu_count() # To see results of parallelizing, set numprocessors\n",
      "# to less than cpu_count().\n",
      "\n",
      "#create an array to store features\n",
      "#nFeature = 13\n",
      "#features_array = np.zeros(len(image_paths), nFeature)\n",
      "\n",
      "numprocessors = cpu_count()\n",
      "\n",
      "# We have to cut up the image_paths list into the number of processes we want to\n",
      "# run. \n",
      "split_image_paths = split_seq(image_paths, numprocessors)\n",
      "\n",
      "# Ok, this block is where the parallel code runs. We time it so we can get a \n",
      "# feel for the speed up.\n",
      "start_time = time()\n",
      "p = Pool(numprocessors)\n",
      "result = p.map_async(extract_features, split_image_paths)\n",
      "poolresult = result.get()\n",
      "end_time = time()\n",
      "#DATA = np.vstack([poolresult[i] for i in range(np.shape(poolresult)[0])])\n",
      "# All done, print timing results.\n",
      "print (\"Finished extracting features. Total time: \" + \n",
      "    str(round(end_time-start_time, 3)) + \" s, or \" + \n",
      "    str( round( (end_time-start_time)/len(image_paths), 5 ) ) + \" s/image.\")\n",
      "# This took about 10-11 seconds on my 2.2 GHz, Core i7 MacBook Pro. It may also\n",
      "# be affected by hard disk read speeds.\n",
      "\n",
      "# To tidy-up a bit, we loop through the poolresult to create a final list of\n",
      "# the feature extraction results for all images.\n",
      "combined_result = []\n",
      "for single_proc_result in poolresult:\n",
      "    for single_image_result in single_proc_result:\n",
      "        combined_result.append(single_image_result)\n",
      "DATA = np.array(combined_result)\n",
      "\n",
      "# DATA contains all the data we wanna train. Now is the training part.\n",
      "X = DATA[:,:-1]\n",
      "Y = DATA[:,-1]\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "clf = RandomForestClassifier(n_estimators=50, n_jobs=-1, \\\n",
      "                             compute_importances=True)\n",
      "clf.fit(X,Y)\n",
      "# save the classifier\n",
      "pickle.dump(X, open('data_X.p'), 'w')\n",
      "pickle.dump(Y, open('data_Y.p'), 'w')\n",
      "pickle.dump(clf, open('trained_classifier_test.p','w'))\n",
      " "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}