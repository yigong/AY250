{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def run_final_classifier(path):\n",
      "    # import\n",
      "#!/usr/bin/env python\n",
      "    import matplotlib.pyplot as plt\n",
      "    from glob import glob\n",
      "    import numpy as np\n",
      "    import scipy.ndimage as ndimg\n",
      "    import skimage.filter as filter\n",
      "    import skimage.transform as transform\n",
      "    import re\n",
      "    from os import listdir\n",
      "    from multiprocessing import Pool, cpu_count\n",
      "    from pylab import imread\n",
      "    from time import time\n",
      "    import pickle\n",
      "    from sklearn.ensemble import RandomForestClassifier\n",
      "    \n",
      "    def row(img2d):\n",
      "        return np.shape(img2d)[0]\n",
      "        \n",
      "    def col(img2d):\n",
      "        return np.shape(img2d)[1]\n",
      "    \n",
      "    def layer_mean(img2d):\n",
      "        img1d = img2d[img2d<255]\n",
      "        return img1d.mean()\n",
      "    \n",
      "    def hist_max(img2d):\n",
      "        img1d = img2d[img2d<255]\n",
      "        [n, bins, patches]= plt.hist(img1d, np.arange(255))\n",
      "        return bins[n.argmax()]\n",
      "    \n",
      "    def edge_length(img2d):\n",
      "        \"gray input\"\n",
      "        imgsize = float((img2d.shape[0]*img2d.shape[1]))\n",
      "        med_filter = ndimg.median_filter(img2d, size = (5,5))\n",
      "        edges = filter.canny(med_filter,3)\n",
      "        return edges.sum()/imgsize\n",
      "    \n",
      "    def edge_sobel_h(img2d):\n",
      "        \"gray input\"\n",
      "        imgsize = float((img2d.shape[0]*img2d.shape[1]))\n",
      "        med_filter = ndimg.median_filter(img2d, size = (5,5))\n",
      "        edges_h = filter.hsobel(med_filter/255.)\n",
      "        return edges_h.sum()/imgsize\n",
      "    \n",
      "    def edge_sobel_v(img2d):\n",
      "        \"gray input\"\n",
      "        imgsize = float((img2d.shape[0]*img2d.shape[1]))\n",
      "        med_filter = ndimg.median_filter(img2d, size = (5,5))\n",
      "        edges_v = filter.vsobel(med_filter/255.)\n",
      "        return edges_v.sum()/imgsize\n",
      "    \n",
      "    def edge_sobel(img2d):\n",
      "        \"gray input\"\n",
      "        imgsize = float((img2d.shape[0]*img2d.shape[1]))\n",
      "        med_filter = ndimg.median_filter(img2d, size = (5,5))\n",
      "        edges = filter.sobel(med_filter/255.)\n",
      "        return edges.sum()/imgsize\n",
      "    \n",
      "    def houghLine(img2d):\n",
      "        \"gray input\"\n",
      "        med_filter = ndimg.median_filter(img2d, size = (5,5))\n",
      "        edges = filter.sobel(med_filter/255.)\n",
      "        [H,theta,distances] = transform.hough_line(edges);\n",
      "        imgsize = float(len(theta)*len(distances))\n",
      "        return H.sum()/imgsize\n",
      "    \n",
      "    def cate_extract(image_path):\n",
      "        cate_temp = image_path.replace(MYDIRECTORY,'')\n",
      "        cate = re.search(r'/.+?/', cate_temp)\n",
      "        return cate_map[cate.group()[1:-1]]\n",
      "    \n",
      "    def features(image_path):\n",
      "        #red mean\n",
      "        img2d = ndimg.imread(image_path)\n",
      "        img2d_gray = ndimg.imread(image_path, flatten= True)\n",
      "        \n",
      "        row_n = row(img2d_gray)\n",
      "        col_n = col(img2d_gray)\n",
      "        \n",
      "        red_mean = layer_mean(img2d[...,0])\n",
      "        green_mean = layer_mean(img2d[...,1])\n",
      "        blue_mean = layer_mean(img2d[...,2])\n",
      "        gray_mean = layer_mean(img2d_gray)\n",
      "        \n",
      "        red_most = hist_max(img2d[...,0])\n",
      "        green_most = hist_max(img2d[...,1])\n",
      "        blue_most = hist_max(img2d[...,2])\n",
      "        gray_most = hist_max(img2d_gray)\n",
      "        \n",
      "        length = edge_length(img2d_gray)\n",
      "        sobel_h = edge_sobel_h(img2d_gray)\n",
      "        sobel_v = edge_sobel_v(img2d_gray)\n",
      "        sobel = edge_sobel(img2d_gray)\n",
      "        hough = houghLine(img2d_gray)\n",
      "        \n",
      "        cate = cate_extract(image_path)\n",
      "        \n",
      "        return list([row_n, col_n, red_mean, green_mean, blue_mean, gray_mean,\\\n",
      "                         red_most,green_most,blue_most, gray_most,\\\n",
      "                         length, sobel_h, sobel_v, sobel, hough, cate])\n",
      "        \n",
      "    #!/usr/bin/env python\n",
      "    \"\"\"\n",
      "    AY 250 - Scientific Research Computing with Python\n",
      "    Homework Assignment 4 - Parallel Feature Extraction Example\n",
      "    Author: Christopher Klein, Joshua Bloom\n",
      "    \"\"\"\n",
      "    ## CHANGE THIS NEXT LINE!\n",
      "    MYDIRECTORY = path\n",
      "    \n",
      "    # FUNCTION DEFINITIONS\n",
      "    # Quick function to divide up a large list into multiple small lists, \n",
      "    # attempting to keep them all the same size. \n",
      "    def split_seq(seq, size):\n",
      "            newseq = []\n",
      "            splitsize = 1.0/size*len(seq)\n",
      "            for i in range(size):\n",
      "                newseq.append(seq[int(round(i*splitsize)):\n",
      "                    int(round((i+1)*splitsize))])\n",
      "            return newseq\n",
      "    # Our simple feature extraction function. It takes in a list of image paths, \n",
      "    # does some measurement on each image, then returns a list of the image paths\n",
      "    # paired with the results of the feature measurement.\n",
      "    def extract_features(image_path_list):\n",
      "        feature_list = []\n",
      "        for image_path in image_path_list:\n",
      "            image_array = imread(image_path)\n",
      "            ft = features(image_path)\n",
      "            \n",
      "            #ft = image_array.shape # This feature is simple. You can modify this\n",
      "            # code to produce more complicated features and to produce multiple\n",
      "            # features in one function call.\n",
      "            feature_list.append(ft)\n",
      "        return feature_list\n",
      "    ### Main program starts here ###################################################\n",
      "    # We first collect all the local paths to all the images in one list\n",
      "    image_paths = []\n",
      "    imgsList = listdir(MYDIRECTORY)\n",
      "    \n",
      "    # Load test Images\n",
      "    for name in imgsList:\n",
      "        image_paths.append(MYDIRECTORY+'/'+name)\n",
      "    \n",
      "    print 'filename' + 15*' '+ 'predicted_class'\n",
      "    print '_ '*20\n",
      "    # Then, we run the feature extraction function using multiprocessing.Pool so \n",
      "    # so that we can parallelize the process and run it much faster.\n",
      "    #numprocessors = cpu_count() # To see results of parallelizing, set numprocessors\n",
      "    # to less than cpu_count().\n",
      "    \n",
      "    #create an array to store features\n",
      "    #nFeature = 13\n",
      "    #features_array = np.zeros(len(image_paths), nFeature)\n",
      "    \n",
      "    #Using multicores\n",
      "    numprocessors = cpu_count()\n",
      "    \n",
      "    # We have to cut up the image_paths list into the number of processes we want to\n",
      "    # run. \n",
      "    split_image_paths = split_seq(image_paths, numprocessors)\n",
      "    \n",
      "    # Ok, this block is where the parallel code runs. We time it so we can get a \n",
      "    # feel for the speed up.\n",
      "    start_time = time()\n",
      "    p = Pool(numprocessors)\n",
      "    result = p.map_async(extract_features, split_image_paths)\n",
      "    poolresult = result.get()\n",
      "    end_time = time()\n",
      "    #DATA = np.vstack([poolresult[i] for i in range(np.shape(poolresult)[0])])\n",
      "    # All done, print timing results.\n",
      "    print (\"Finished extracting features. Total time: \" + \n",
      "        str(round(end_time-start_time, 3)) + \" s, or \" + \n",
      "        str( round( (end_time-start_time)/len(image_paths), 5 ) ) + \" s/image.\")\n",
      "    # This took about 10-11 seconds on my 2.2 GHz, Core i7 MacBook Pro. It may also\n",
      "    # be affected by hard disk read speeds.\n",
      "    \n",
      "    # To tidy-up a bit, we loop through the poolresult to create a final list of\n",
      "    # the feature extraction results for all images.\n",
      "    combined_result = []\n",
      "    for single_proc_result in poolresult:\n",
      "        for single_image_result in single_proc_result:\n",
      "            combined_result.append(single_image_result)\n",
      "    X_test = np.array(combined_result)\n",
      "    # X_test are the features of the test images\n",
      "    # load the trained classifier\n",
      "    clf = pickle.load(open('trained_classifier.p', 'rb'))\n",
      "    #load the category table\n",
      "    cate_map = dict(zip(range(len(categories)),categories))\n",
      "    # predict the category for the test image\n",
      "    Y_pred_num = clf.predict(X_test)\n",
      "    #printing\n",
      "    for i in len(Y_pred_num):\n",
      "        filename = image_paths[i].replace(path,'')\n",
      "        print filename[1:]+ ' '*6 + cate_map[Y_pred_num[i]]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}